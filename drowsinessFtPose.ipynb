{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '~/anaconda3/bin/python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from asyncio import transports\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import base64\n",
    "# import asyncio\n",
    "# from websockets.sync.client import connect\n",
    "\n",
    "# percentage detection\n",
    "EYE_ASPECT_RATIO_TRESHOLD = 0.17\n",
    "EYE_ASPECT_RATIO_CONSEC_FRAMES = 5\n",
    "MOUTH_ASPECT_RATIO_TRESHOLD = 55\n",
    "MOUTH_ASPECT_RATIO_CONSEC_FRAMES = 15\n",
    "\n",
    "COUNTER_EYE = 0\n",
    "COUNTER_MOUTH = 0\n",
    "\n",
    "IS_MOUTH_OPEN_15 = False\n",
    "IS_EYE_CLOSE_5 = False\n",
    "IS_DROWSINESS = False\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('./assets/haarcascade_frontalface_default.xml')\n",
    "phone_cascade = cv2\n",
    "\n",
    "class poseDetector:\n",
    "\n",
    "    def __init__(self, mode=False, modelComplexity=1, smLm=True, enaSeg=False, smSeg=True, minDetectConfi=0.5,\n",
    "                 minTrackConfi=0.5):\n",
    "        self.static_image_mode = mode\n",
    "        self.model_complexity = modelComplexity\n",
    "        self.smooth_landmarks = smLm\n",
    "        self.enable_segmentation = enaSeg\n",
    "        self.smooth_segmentation = smSeg\n",
    "\n",
    "        self.min_detection_confidence = minDetectConfi\n",
    "        self.min_tracking_confidence = minTrackConfi\n",
    "\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose # calling\n",
    "        self.pose = self.mpPose.Pose(self.static_image_mode, self.model_complexity, self.smooth_landmarks,\n",
    "        self.enable_segmentation, self.smooth_segmentation, self.min_detection_confidence,\n",
    "        self.min_tracking_confidence)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks:\n",
    "            if draw:\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, draw=True):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\n",
    "                    cv2.putText(img, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)  # Adjusted y-coordinate\n",
    "        return lmList\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
    "        return distance\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    eyes = (A+B) / (2 * C)\n",
    "    return eyes\n",
    "def yawn_aspect_ratio(mouth):\n",
    "    # pointing mouth \n",
    "    distYawn = math.sqrt((math.pow(mouth[10][0] - mouth[2][0], 2) + math.pow(mouth[10][1] - mouth[2][1], 2)))\n",
    "    print(distYawn)\n",
    "    return distYawn\n",
    "\n",
    "# def get_image_video(video):\n",
    "#     cv2.imwrite(\"frame%d.jpg\" % 1, video) \n",
    "#     with open(\"frame1.jpg\", \"rb\") as image_file:\n",
    "#         encoded_string = base64.b64encode(image_file.read())\n",
    "#         socket_connect(encoded_string)\n",
    "\n",
    "class SocketTrigger:\n",
    "    # import socketio\n",
    "    # sio = socketio.Client()\n",
    "    # sio.connect('wss://0gw901vv-3100.asse.devtunnels.ms/', transports=['websocket'])\n",
    "    # sio.emit('message', 'test')\n",
    "    # print('my sid is', sio.sid)\n",
    "    \n",
    "    # import socketio\n",
    "\n",
    "    # sio = socketio.Client()\n",
    "    \n",
    "\n",
    "    def save_image(image):\n",
    "        cv2.imwrite(\"./drow/drowImg%d.jpg\" % 1, image)\n",
    "        # import requests\n",
    "        # url = 'http://file.api.wechat.com/cgi-bin/media/upload?access_token=ACCESS_TOKEN&type=TYPE'\n",
    "        # files = {'media': open('./nodeServer/drow/drowImg%d.jpg', 'rb')}\n",
    "        # requests.post(url, files=files) \n",
    "        # with connect(\"wss://0gw901vv-3100.asse.devtunnels.ms/\") as websocket:\n",
    "        #     cv2.imwrite(\"frame%d.jpg\" % 1, video) \n",
    "        #     with open(\"frame1.jpg\", \"rb\") as image_file:\n",
    "        #         encoded_string = base64.b64encode(image_file.read())\n",
    "        #         websocket.send(encoded_string)\n",
    "        #         message = websocket.recv()\n",
    "        #         print(f\"Received: {message}\")\n",
    "\n",
    "# socket_connect('ok')\n",
    "\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# poseDetector = poseDetector()\n",
    "\n",
    "predictor = dlib.shape_predictor('./assets/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS['left_eye']\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS['right_eye']\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "\n",
    "# video_file_path = './assets/videofajarngantukgajelas.mp4'\n",
    "\n",
    "# video_capture = cv2.VideoCapture(video_file_path) # video history\n",
    "video_capture = cv2.VideoCapture(0) # video live webcam\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    # get_image_video(frame)\n",
    "    image = frame\n",
    "    # cv2.imwrite(\"frame_not_preprocessing.jpg\", image)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if time.time() - start_time > 15:  # Check if 15 seconds have elapsed\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # IMAGE PREPROCESSING\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # cv2.imshow('frame', gray)\n",
    "    # cv2.imshow('fram2', frame)\n",
    "    faces = detector(gray, 0)\n",
    "    face_rectangle = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    # img = poseDetector.findPose(frame)\n",
    "    # lmList = poseDetector.findPosition(img, draw=False)\n",
    "    \n",
    "    # if len(lmList) > 0:\n",
    "    #     for lm in lmList:\n",
    "    #         cv2.putText(frame, str(lm[0]), (lm[1], lm[2]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    for(x,y,w,h) in face_rectangle:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255,0,0), 2)\n",
    "        \n",
    "    # if len(lmList) > 16:  # Ensure enough landmarks are detected\n",
    "    #     # hand_landmark = lmList[16][1:]  # Hand landmark\n",
    "    #     # eye_landmark = lmList[8][1:]  # Eye landmark\n",
    "    #     handIdxR = lmList[19][1:]\n",
    "    #     earR = lmList[7][1:]\n",
    "    #     handIdxL = lmList[20][1:]\n",
    "    #     earL = lmList[7][1:]\n",
    "        \n",
    "    #     if (handIdxR and earR) or (handIdxL and earL):\n",
    "    #         distancePose = poseDetector.calculate_distance(handIdxR, earR)\n",
    "    #         # distancePose = poseDetector.calculate_distance(wristR, earR)\n",
    "    #         distance_threshold = 100  # Adjust this threshold as needed\n",
    "\n",
    "    #         if distancePose < distance_threshold:\n",
    "    #             cv2.putText(img, \"Making a phone call\", (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    #         # Calculate the position of the text based on the original image size\n",
    "    #         h, w, _ = img.shape\n",
    "    #         text_position = (20, h - 50)\n",
    "    #         # Show the calculated distance\n",
    "    #         cv2.putText(img, f\"Distance: {distancePose:.2f}\", text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    \n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        mouth = shape[mStart:mEnd]\n",
    "        \n",
    "        leftEyeAspectRatio = eye_aspect_ratio(leftEye)\n",
    "        rightEyeAspectRatio = eye_aspect_ratio(rightEye)\n",
    "        \n",
    "        eyeAspectRatio = (leftEyeAspectRatio + rightEyeAspectRatio) / 2\n",
    "        mouthAspectRatio = yawn_aspect_ratio(mouth)\n",
    "        \n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        mouthHull = cv2.convexHull(mouth)\n",
    "        \n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0,255,0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0,255,0), 1)\n",
    "        cv2.drawContours(frame, [mouthHull], -1, (0,0,255), 1)\n",
    "        print(mouthAspectRatio)\n",
    "        cv2.putText(frame, f'{eyeAspectRatio}', (250, 400), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,0), 2)\n",
    "        if eyeAspectRatio < EYE_ASPECT_RATIO_TRESHOLD:\n",
    "            COUNTER_EYE += 1\n",
    "            print(COUNTER_EYE)\n",
    "            if COUNTER_EYE >= EYE_ASPECT_RATIO_CONSEC_FRAMES:\n",
    "                IS_EYE_CLOSE_5 = True\n",
    "                cv2.putText(frame, 'Mata Beler!', (150, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,255), 2)\n",
    "        else:\n",
    "            IS_EYE_CLOSE_5 = False\n",
    "            COUNTER_EYE = 0\n",
    "        \n",
    "        if mouthAspectRatio > MOUTH_ASPECT_RATIO_TRESHOLD:\n",
    "            COUNTER_MOUTH += 1\n",
    "            if COUNTER_MOUTH >= MOUTH_ASPECT_RATIO_CONSEC_FRAMES:\n",
    "                IS_MOUTH_OPEN_15 = True\n",
    "                cv2.putText(frame, 'Mouth Yawn!', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,255), 2)\n",
    "        else:\n",
    "            IS_MOUTH_OPEN_15 = False\n",
    "            COUNTER_MOUTH = 0\n",
    "            \n",
    "        if IS_MOUTH_OPEN_15 or IS_EYE_CLOSE_5:\n",
    "            IS_DROWSINESS = True\n",
    "        else:\n",
    "            IS_DROWSINESS = False\n",
    "        \n",
    "        if IS_DROWSINESS:\n",
    "            # SocketTrigger.save_image(image)\n",
    "            cv2.putText(frame, 'Anda Ngantuk!', (250, 300), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,0), 2)\n",
    "            \n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
